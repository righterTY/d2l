1- 多层感知机
先用全连接线性模型，验证训练集测试集是否出现数据处理错误
1定义损失函数
2定义网络结构：（函数）get_net
   input：特征个数in_features
3损失函数选用均方根误差，避免在不同的真实情况（相差很大）的情况下，预测产生的预测
  偏差影响不同
   将小于1的值设置为1（clipped_preds）；计算均方根误差（remse）
4训练（train）参数：网络结构，训练特征and标签，测试特征and标签，轮次，学习率，权
  重，批次
   定义train_ls, test_ls 存储损失
   a.创建数据的迭代器d2l.load_array()
   b.使用Adam优化算法
   c.外循环（轮次），内循环（批次(数据)）
       每次反向传播都将梯度重新归0

数据集结构：
X:                             y:
     x1 x2 x3 ... xm           y
0                              0
1                              1
...                             ...
n                              n

K折交叉get_k_fold_data
原始数据分成K个不重叠子集，执行K次模拟训练和验证。每次在K-1个子集上训练，剩余一个子
集上验证。
切片slice(), 切成k个子集。把第i个子集作为验证集x_valid, y_valid
其余的都是训练集


2- 层和块。块：由多个层组成的组件或模型本身
1. nn.Sequential()定义了一种特殊的Module，可表示一个块的类
2. 用类（class）来写
3. 在前向传播模块（作用：根据输入x返回所需的模型输出）中执行控制流。
4. 混搭组合
    两层，全连接(20,64)(64,32)，激活函数relu
    最后一层(32,16)

question 1： 两个块，用nn.Sequential(),每个块两层（全连接，激活函数relu和tanh函数）
   将两个块的结果串在一起实现前向传播，用torch.cat()


3- 参数
a.  通过Sequential类定义模型，可用索引访问模型的任意层
    1 第二层的所有参数: net[2].state_dict()
    2 第二层的偏置： net[2].bias    net[2].bias,data
   一次访问所有参数：
     第一层的参数： net[0].named_parameters()， 迭代器，返回字典
      所有层的参数： net.named_parameters(),  迭代器，返回字典（可用于不是Sequential定义的模型）
b. 初始化参数
   内置初始化器 init_normal(m)
     1 先判断类型是否为 nn.Linear()
     2 调用nn.init.normal_ 和 nn.init.zeros_ 初始化权重和偏置
     3 net.appply() 传入参数
     调用不同的块及其嵌套，用中括号 [ ]
c. 以上好像只对Sequential类有用？？？

保存模型
torch.save()  torch.load()
class xxx(nn.Module):
    """模型"""
net = xxx()
torch.save(net.state_dict(), 'xxx,params')
clone = xxx()
clone.load_state_dict(torch.load('xxx.params'))

gpu上计算1000个100*100矩阵乘法所需时间，计算输出矩阵F范数。
类Timer()记录时间


4- 卷积神经网络
a.卷积
输入矩阵X: n*n，卷积核K: k*k
输出大小：(n-k+1)*(n-k+1)
2维卷积： nn.Conv2d(1, 1, kernel_size=(), bias = False)
该二维卷积层使用四维输入和输出格式（批量大小、通道、高度、宽度）
X = X.reshape((1, 1, 1, 1))

b.填充and步长padding stride
填充 p*p： (n-k+2p+1)*(n-k+2p+1)
步长     s： [(n-k+2p)/s + 1]*[(n-k+2p)/s + 1]

c.多通道的卷积操作
输入多个通道：有几个通道就对应几层卷积核（2（通道数）*2*2）。对每个通道分别作卷积，然后求和
输出多个通道：有几个通道就对应几个卷积核（3（个数）*2*（通道数）*2*2）

d. 汇聚层（就是池化层！）
最大池化（max pooling）
平均池化（average pooling
池化的卷积核： k*k; 步长：s    [(n-k)/s + 1]*[(n-k)/s + 1]

池化层的输出通道数与输入通道数相同


5- LeNet
卷积层+全连接层
layer1 卷积层： 5*5卷积核（6个），填充2，sigmoid激活函数
layer2 池化层： 2*2平均池化，步长2
layer3: 卷积层： 5*5卷积核（16个），sigmoid激活函数
layer4: 池化层： 2*2平均池化，步长2
layer5: 全连接120
layer6: 同上84
layer7: 同上10

初始化权重init_weights()
在gpu上运算net.to(device)
优化器SGD
损失函数：交叉熵nn.CrossEntropyLoss()



6- Alexnet
layer1 卷积层： 11*11卷积核（96），步长4
layer2 池化层： 3*3最大池化，步长2
layer3 卷积层： 5*5卷积核（256），填充2
layer4 池化层： 3*3最大池化，步长2
layer5 卷积层： 3*3卷积核（384），填充1
layer6 卷积层： 3*3卷积核（384），填充1
layer7 卷积层： 3*3卷积核（256），填充1
layer8 池化层： 3*3最大池化，步长2
layer9 全连接层4096
layer10 全连接层4096
layer11 全连接层1000
激活函数：nn.ReLu()
全连接层之间，用dropout来避免过拟合 nn.Dropout(p=0.5)，超参数p设为0.5


7- VGGnet
VGG块： 1.卷积层k=3, p=1； 2.最大池化层k=2,s=2
     vgg_block(卷积层数， 输入数， 输出数)：生成VGG块
     用for循环：
        1.用列表存储每一层
        2.返回Sequential容器

VGG-11:
vgg(conv_arch)
conv_arch: 元组，（块中卷积层的个数，输出数）
conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))

结构：
VGG块1：3*3卷积层, p=1（64） 初始输入图像大小224*224*1
               2*2最大池化层, s=2
VGG块2：3*3卷积层, p=1（128）
               3*3卷积层, p=1（128）
               2*2最大池化层, s=2
VGG块3：3*3卷积层, p=1（256）
               3*3卷积层, p=1 （256）
               2*2最大池化层, s=2
VGG块4：3*3卷积层, p=1（512）
               3*3卷积层, p=1（512）
               2*2最大池化层, s=2
VGG块5：3*3卷积层, p=1（512）
               3*3卷积层, p=1（512）
               2*2最大池化层, s=2     最终输出图像大小：7*7*512
全连接层：*3

激活函数relu(), 全连接层使用dropout(0.5)


8- NiN
NiN块：n*n卷积层，s, p    自定义初始化n,s,p
             1*1卷积层，s=1, p=0
             1*1卷积层，s=1, p=0
nin_block(in_channels, out_channels, kernel_size, strides, padding)


结构：
NiN块1：5*5卷积层(96)，s=4, p=0
             1*1卷积层，s=1, p=0
             1*1卷积层，s=1, p=0
池化层：3*3最大池化， s=2
NiN块2：n*n卷积层，s, p    自定义初始化n,s,p
             1*1卷积层，s=1, p=0
             1*1卷积层，s=1, p=0
池化层：3*3最大池化， s=2
NiN块3：n*n卷积层，s, p    自定义初始化n,s,p
             1*1卷积层，s=1, p=0
             1*1卷积层，s=1, p=0
池化层：3*3最大池化， s=2
NiN块4：n*n卷积层，s, p    自定义初始化n,s,p
             1*1卷积层，s=1, p=0
             1*1卷积层，s=1, p=0
池化层：3*3平均池化，p=1


9- GoogleNet
Inception块：四条并行路径
第一条：1*1卷积层
第二条：1*1卷积层； 3*3卷积层，p=1
第三条：1*1卷积层； 5*5卷积层，p=2
第四条：3*3最大池化，p=1；1*1卷积层
最后把四条路径通道合并

Inception(输入通道，输出通道1，输出通道2，输出通道3，输出通道4)
输出通道2、3为元组，包含两层的输出通道数

每一层激活函数：nn.Relu()


结构：
7*7卷积层（64），s=2,p=3
3*3最大池化层，s=2,p=1
1*1卷积层（64）
3*3卷积层（192），p=1
3*3最大池化层，s=2,p=1
Inception块(两个)
3*3最大池化层，s=2,p=1
Inception块(五个)
3*3最大池化层，s=2,p=1
Inception块(两个)
全局平均池化层
全连接层


10- Batch_Norm(批量规范化)
全连接层：将批量规范化置于仿射变换和激活函数之间
卷积层：同上，












